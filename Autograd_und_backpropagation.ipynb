{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6066ac9e",
   "metadata": {},
   "source": [
    "## AutoGrad in PyTorch\n",
    "\n",
    "In diesem Praxisbeispiel schauen wir uns an, wie PyTorch mittels AutoGrad die Gradienten eines Modells berechnet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "977a706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import grad\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c542d6",
   "metadata": {},
   "source": [
    "Torch wird jeden Tensor, der den Parameter **requires_grad** auf **True** gesetzt hat als Variable betrachten, die es zu optimieren gilt. Ist dieser Wert nicht gesetzt, wird der Tensor als konstant angenommen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc05aed",
   "metadata": {},
   "source": [
    "Wir definieren uns ein Minimal-Neuronales-Netz mit einem Eingangsneuron:\n",
    "a = ReLU(wx+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "923df09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([3.])\n",
    "w = torch.tensor([2.], requires_grad=True)\n",
    "b = torch.tensor([1.], requires_grad=True)\n",
    "\n",
    "# Inference mit x=3.\n",
    "m = w*x + b\n",
    "a = F.relu(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88822056",
   "metadata": {},
   "source": [
    "Wir wollen in diesem Fall die Einheitsfunktion als Loss-Funktion verwenden (anders gesagt: Wir verwenden keine Loss-Funktion, sondern wollen den Output des Netzwerks minimieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "881ab846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ReluBackward0 at 0x10cd73bb0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "307d94d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AddBackward0 at 0x107ba1850>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dd96e8",
   "metadata": {},
   "source": [
    "## Aufgabe 1: Erstellen Sie einen SGD-Optimizer und setzen Sie die Gradienten der Parameter zu Null.\n",
    "\n",
    "Die Lernrate des Optimizers soll 0.001 sein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977445b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0b2ed0",
   "metadata": {},
   "source": [
    "### Führen Sie nun die Backpropagation von a mit der backward()-Funktion durch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f3d95c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f969418",
   "metadata": {},
   "source": [
    "### Schauen Sie, welche Gradienten in den Parametern w und b aufgelaufen sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9425c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44314818",
   "metadata": {},
   "source": [
    "Sie sollten sehen: \n",
    "- w.grad: tensor([3.])\n",
    "- b.grad: tensor([1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e233bd90",
   "metadata": {},
   "source": [
    "### Führen Sie nochmals eine Inference und einen Backward-Pass durch und schauen Sie erneut die Gradienten an.\n",
    "\n",
    "Was erkennen wir dabei?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a6e3b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "328325f2",
   "metadata": {},
   "source": [
    "Sie sollten erkennen können, dass sich die Gradienten jetzt verdoppelt haben:\n",
    "- w.grad: tensor([6.])\n",
    "- b.grad: tensor([2.])\n",
    "\n",
    "Deswegen müssen wir die Gradienten am Anfang eines Batch (nach erfolgtem Update) zu Null setzen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acf591d",
   "metadata": {},
   "source": [
    "## Aufgabe 2: Welche Veränderung erwarten Sie für beide Parameter nach diesem Schritt des Optimizers? \n",
    "\n",
    "Berechnen Sie diese von Hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1d84e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "erwartet_w = \n",
    "erwartet_b = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d83fddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "050e0146",
   "metadata": {},
   "source": [
    "## Aufgabe 3: Führen Sie nun optim.step() durch und kontrollieren Sie, ob Sie richtig lagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcbe135",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152f3a18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "=/Users/marcaubreville/miniconda_m1/bin/python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
